encoder:
  image_size: 224
  patch_size: 4
  in_channels: 1
  embed_dim: 64
  depths: [2, 2, 6, 2]
  num_heads: [2, 4, 8, 16]
  window_size: 7
  mlp_ratio: 4.0
  drop_rate: 0.0
  attn_drop_rate: 0.0
  drop_path_rate: 0.1

decoder:
  vocab_size: 600
  max_seq_len: 512
  d_model: 256
  nhead: 8
  num_layers: 4
  dim_feedforward: 1024
  dropout: 0.1

train:
  batch_size: 64
  num_epochs: 100
  learning_rate: 1.0e-4
  weight_decay: 0.01
  warmup_steps: 2000
  label_smoothing: 0.1
  grad_clip: 1.0
  fp16: true
  num_workers: 4
  save_every: 10
  eval_every: 1
  patience: 15
  checkpoint_dir: checkpoints/pretrain
  log_dir: logs/pretrain
  wandb_project: null

data:
  train_dir: data/processed/train
  val_dir: data/processed/val
  test_dir: data/processed/test
  vocab_path: model/vocab.json
  image_size: 224
  max_seq_len: 512
