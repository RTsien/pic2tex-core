# Training config optimized for Apple M-series chips (MPS backend)
#
# Key differences from CPU config:
#   - Larger batch_size (MPS unified memory allows bigger batches)
#   - fp16: false (MPS has limited FP16 support; FP32 is more stable)
#   - More workers for data loading (M-series has efficient cores)
#   - Faster convergence expected (~3-5x speedup over CPU)
#
# Usage:
#   python -m model.train --config model/configs/mps_train.yaml
#   python -m model.train --config model/configs/mps_train.yaml --device mps

encoder:
  image_size: 224
  patch_size: 4
  in_channels: 1
  embed_dim: 64
  depths: [2, 2, 6, 2]
  num_heads: [2, 4, 8, 16]
  window_size: 7
  mlp_ratio: 4.0
  drop_rate: 0.0
  attn_drop_rate: 0.0
  drop_path_rate: 0.1

decoder:
  vocab_size: 356
  max_seq_len: 256
  d_model: 256
  nhead: 8
  num_layers: 4
  dim_feedforward: 1024
  dropout: 0.1

train:
  batch_size: 32
  num_epochs: 50
  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_steps: 300
  label_smoothing: 0.1
  grad_clip: 1.0
  fp16: false
  num_workers: 4
  save_every: 5
  eval_every: 1
  patience: 10
  checkpoint_dir: checkpoints
  log_dir: logs
  wandb_project: null

data:
  train_dir: data/processed/train
  val_dir: data/processed/val
  test_dir: data/processed/test
  vocab_path: model/vocab.json
  image_size: 224
  max_seq_len: 256
